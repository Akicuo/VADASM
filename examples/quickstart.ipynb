{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V-ADASM Quickstart Notebook\n",
        "\n",
        "**V-ADASM**: Vision-Adaptive Dimensionality-Aligned Subspace Merging\n",
        "\n",
        "This notebook demonstrates how to use V-ADASM to merge a large multimodal model (donor) with a small text model (base) to create a compact Vision-Language Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Install V-ADASM\n",
        "!pip install -e ..\n",
        "\n",
        "# Or from GitHub:\n",
        "# !pip install git+https://github.com/yourorg/vadasm.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "import torch\n",
        "from vadasm.merger import VADASMMerger, ModelConfig, MergeConfig\n",
        "\n",
        "# 1. Configure Models\n",
        "small_config = ModelConfig(\n",
        "    name_or_path=\"microsoft/phi-2\",  # Small base (2.7B params)\n",
        "    is_moe=False,\n",
        "    has_vision=False\n",
        ")\n",
        "\n",
        "large_config = ModelConfig(\n",
        "    name_or_path=\"llava-hf/llava-1.5-7b-hf\",  # Large donor with vision\n",
        "    is_moe=False,\n",
        "    has_vision=True\n",
        ")\n",
        "\n",
        "# 2. Configure Merge Process\n",
        "merge_config = MergeConfig(\n",
        "    fusion_beta=0.3,        # Vision delta weight\n",
        "    projector_svd_rank=0.95, # SVD variance threshold\n",
        "    ties_drop_rate=0.3,     # TIES sparsification\n",
        "    evo_generations=15,     # Evolutionary optimization\n",
        "    moe_top_k=2,           # For MoE models\n",
        "    device=\"cuda\"          # GPU acceleration\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# 3. Initialize Merger\n",
        "merger = VADASMMerger(merge_config)\n",
        "\n",
        "print(\"üöÄ Starting V-ADASM merge pipeline...\")\n",
        "print(f\"Small model: {small_config.name_or_path}\")\n",
        "print(f\"Large model: {large_config.name_or_path}\")\n",
        "print(f\"Output size: ~{small_config.name_or_path.split('-')[-1]} params (no size bloat!)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# 4. Perform Training-Free Merge!\n",
        "merged_model = merger.merge_models(small_config, large_config)\n",
        "\n",
        "print(\"‚úÖ Merge complete!\")\n",
        "print(f\"Model has vision capability: {merged_model.config.has_vision}\")\n",
        "print(f\"Parameter count preserved: {sum(p.numel() for p in merged_model.parameters())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# 5. Test Multimodal Inference\n",
        "from transformers import pipeline\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "# Load merged model for inference\n",
        "vlm = pipeline(\"image-to-text\", model=merged_model, trust_remote_code=True)\n",
        "\n",
        "# Example image (you can use your own)\n",
        "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"\n",
        "image = Image.open(requests.get(image_url, stream=True).raw)\n",
        "\n",
        "# Multimodal generation\n",
        "prompt = \"Describe this scene in detail:\"\n",
        "output = vlm(prompt, images=[image], max_new_tokens=100)[0]['generated_text']\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Response: {output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# 6. Optional: Evaluate Performance\n",
        "!python ../scripts/eval_vlm.py --model ./merged_model --tasks vqav2 mmlu --limit 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "- **Fork and customize**: Modify hyperparams, try different model pairs\n",
        "- **Scale up**: Use larger donor models (LLaVA-34B, Gemini) for better vision\n",
        "- **Deploy**: Export to ONNX/TensorRT for edge inference\n",
        "- **Contribute**: Add support for new architectures, benchmarks\n",
        "\n",
        "**Key Benefits:**\n",
        "- üß† **Same size**: Output = small model parameters  \n",
        "- üöÄ **Training-free**: Offline merge in ~2-4 hours\n",
        "- üëÅÔ∏è **Multimodal**: Text-only input ‚Üí Text+Image processing\n",
        "- üîß **Extensible**: Add new fusion methods, optimizers\n",
        "\n",
        "Visit [GitHub](https://github.com/yourorg/vadasm) for more details!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}